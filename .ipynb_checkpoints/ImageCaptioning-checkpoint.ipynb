{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015e710-235e-4c54-bb33-9e18d01ced0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is a refactored, production-quality implementation of the image captioning project.\n",
    "#\n",
    "# --- NEW in this version: Full Model Fine-Tuning ---\n",
    "# Based on the user's request, this version removes all layer-freezing logic. The entirety\n",
    "# of both encoders (Swin and ViT) and both decoders (T5-small) are now fully fine-tuned.\n",
    "# This grants the models maximum flexibility to adapt to the dataset, though it also\n",
    "# significantly increases the risk of overfitting.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from torchvision import transforms\n",
    "from transformers import (\n",
    "    SwinModel, ViTModel, T5ForConditionalGeneration, AutoImageProcessor, AutoTokenizer, AutoConfig\n",
    ")\n",
    "from PIL import Image\n",
    "import evaluate\n",
    "import warnings\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from zipfile import BadZipFile\n",
    "import shutil\n",
    "\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "IMAGES_PATH = \"/content/flickr8k_data/Images\"\n",
    "CAPTIONS_FILE = \"/content/flickr8k_data/captions.txt\"\n",
    "\n",
    "# --- Model & Training Hyperparameters ---\n",
    "ENCODER_SWIN = \"microsoft/swin-base-patch4-window7-224-in22k\"\n",
    "ENCODER_VIT = \"google/vit-base-patch16-224-in21k\"\n",
    "DECODER_NAME = \"t5-small\"\n",
    "\n",
    "TOTAL_EPOCHS = 40\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 0.2\n",
    "MAX_LENGTH = 50\n",
    "BEAM_WIDTH = 10\n",
    "T5_DROPOUT_RATE = 0.5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f723cb6-4ef6-4919-98bd-fa47ac5a3889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Flickr8k Dataset ---\n",
    "class Flickr8kDataset(Dataset):\n",
    "    def __init__(self, images_path, captions_file, image_processor, tokenizer):\n",
    "        self.images_path = images_path\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.captions_map = defaultdict(list)\n",
    "        self.image_files = []\n",
    "        with open(captions_file, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline()\n",
    "            if 'image,caption' not in first_line: self._process_line(first_line)\n",
    "            for line in f: self._process_line(line)\n",
    "    def _process_line(self, line):\n",
    "        parts = line.strip().split(',', 1)\n",
    "        if len(parts) < 2: return\n",
    "        image_file, caption = parts\n",
    "        if image_file not in self.captions_map: self.image_files.append(image_file)\n",
    "        self.captions_map[image_file].append(caption.strip())\n",
    "    def __len__(self): return len(self.image_files)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.images_path, image_name)\n",
    "        try: image = Image.open(image_path).convert(\"RGB\")\n",
    "        except FileNotFoundError: image = Image.new('RGB', (224, 224), color='red')\n",
    "        pixel_values = self.image_processor(image, return_tensors=\"pt\").pixel_values\n",
    "        caption = self.captions_map[image_name][0]\n",
    "        labels = self.tokenizer(caption, padding=\"max_length\", max_length=MAX_LENGTH, truncation=True, return_tensors=\"pt\").input_ids\n",
    "        return {\"pixel_values\": pixel_values.squeeze(0), \"labels\": labels.squeeze(0),\n",
    "                \"image_id\": image_name, \"raw_captions_list\": self.captions_map[image_name]}\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    image_ids = [item['image_id'] for item in batch]\n",
    "    raw_captions_lists = [item['raw_captions_list'] for item in batch]\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels, \"image_id\": image_ids, \"raw_captions_list\": raw_captions_lists}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90686d-e70e-470b-872c-fc5230ccdf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Model Architecture ---\n",
    "class MappingLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        return self.proj(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e61998-4224-4ae2-b7a4-240b19ff8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Training and Evaluation Functions ---\n",
    "def train_model(encoder, decoder, mapping, train_loader, val_loader, optimizer, device, epochs, model_name=\"\"):\n",
    "    print(f\"\\n--- Training {model_name} ---\")\n",
    "    for epoch in range(epochs):\n",
    "        encoder.train(); decoder.train(); mapping.train()\n",
    "        total_loss = 0\n",
    "        desc = f\"Training {model_name} Epoch {epoch + 1}/{epochs}\"\n",
    "        for batch in tqdm(train_loader, desc=desc):\n",
    "            optimizer.zero_grad()\n",
    "            pixel_values, labels = batch[\"pixel_values\"].to(device), batch[\"labels\"].to(device)\n",
    "            encoder_outputs = encoder(pixel_values=pixel_values)\n",
    "            mapped_encoder_outputs = mapping(encoder_outputs.last_hidden_state)\n",
    "            outputs = decoder(encoder_outputs=(mapped_encoder_outputs,), labels=labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        encoder.eval(); decoder.eval(); mapping.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n",
    "                pixel_values, labels = batch[\"pixel_values\"].to(device), batch[\"labels\"].to(device)\n",
    "                encoder_outputs = encoder(pixel_values=pixel_values)\n",
    "                mapped_encoder_outputs = mapping(encoder_outputs.last_hidden_state)\n",
    "                outputs = decoder(encoder_outputs=(mapped_encoder_outputs,), labels=labels)\n",
    "                total_val_loss += outputs.loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} -> Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b03951-d921-4775-9f92-c1a3843cbc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ensemble(\n",
    "    encoder1, decoder1, mapping1, image_processor1,\n",
    "    encoder2, decoder2, mapping2, image_processor2,\n",
    "    dataloader, tokenizer, device, split_name\n",
    "):\n",
    "    print(f\"\\n--- Evaluating Ensemble on {split_name} set ---\")\n",
    "    encoder1.eval(); decoder1.eval(); mapping1.eval()\n",
    "    encoder2.eval(); decoder2.eval(); mapping2.eval()\n",
    "\n",
    "    predictions_map, ground_truths_map = {}, {}\n",
    "    gen_kwargs = {\"max_length\": MAX_LENGTH, \"num_beams\": BEAM_WIDTH, \"return_dict_in_generate\": True, \"output_scores\": True}\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        for batch in tqdm(dataloader, desc=f\"Evaluating Ensemble on {split_name}\"):\n",
    "            image_ids = batch[\"image_id\"]\n",
    "            raw_captions = batch[\"raw_captions_list\"]\n",
    "\n",
    "            images = []\n",
    "            for img_id in image_ids:\n",
    "                try:\n",
    "                    img_path = os.path.join(IMAGES_PATH, img_id)\n",
    "                    images.append(Image.open(img_path).convert(\"RGB\"))\n",
    "                except FileNotFoundError:\n",
    "                    images.append(Image.new('RGB', (224, 224), color='red'))\n",
    "\n",
    "            pixel_values1 = image_processor1(images, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "            pixel_values2 = image_processor2(images, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                from transformers.modeling_outputs import BaseModelOutput\n",
    "\n",
    "                encoder_outputs1 = encoder1(pixel_values=pixel_values1)\n",
    "                mapped1 = mapping1(encoder_outputs1.last_hidden_state)\n",
    "                encoder1_for_generate = BaseModelOutput(last_hidden_state=mapped1)\n",
    "                outputs1 = decoder1.generate(encoder_outputs=encoder1_for_generate, **gen_kwargs)\n",
    "\n",
    "                encoder_outputs2 = encoder2(pixel_values=pixel_values2)\n",
    "                mapped2 = mapping2(encoder_outputs2.last_hidden_state)\n",
    "                encoder2_for_generate = BaseModelOutput(last_hidden_state=mapped2)\n",
    "                outputs2 = decoder2.generate(encoder_outputs=encoder2_for_generate, **gen_kwargs)\n",
    "\n",
    "            captions1 = tokenizer.batch_decode(outputs1.sequences, skip_special_tokens=True)\n",
    "            captions2 = tokenizer.batch_decode(outputs2.sequences, skip_special_tokens=True)\n",
    "            scores1 = outputs1.sequences_scores\n",
    "            scores2 = outputs2.sequences_scores\n",
    "\n",
    "            for i, image_id in enumerate(image_ids):\n",
    "                score1, score2 = scores1[i].item(), scores2[i].item()\n",
    "                final_caption = captions1[i].strip() if score1 > score2 else captions2[i].strip()\n",
    "                predictions_map[image_id] = final_caption if final_caption else \".\"\n",
    "                if image_id not in ground_truths_map: ground_truths_map[image_id] = raw_captions[i]\n",
    "\n",
    "    gts_coco_format = {img_id: [{'caption': c} for c in caps] for img_id, caps in ground_truths_map.items()}\n",
    "    res_coco_format = {img_id: [{'caption': cap}] for img_id, cap in predictions_map.items()}\n",
    "    ptb_tokenizer = PTBTokenizer()\n",
    "    gts_tokenized, res_tokenized = ptb_tokenizer.tokenize(gts_coco_format), ptb_tokenizer.tokenize(res_coco_format)\n",
    "    cider_scorer, spice_scorer = Cider(), Spice()\n",
    "    cider_score, _ = cider_scorer.compute_score(gts_tokenized, res_tokenized)\n",
    "    spice_score, _ = spice_scorer.compute_score(gts_tokenized, res_tokenized)\n",
    "\n",
    "    preds_list, refs_list = list(predictions_map.values()), [ground_truths_map[k] for k in predictions_map.keys()]\n",
    "    bleu, rouge, meteor = evaluate.load(\"bleu\"), evaluate.load(\"rouge\"), evaluate.load(\"meteor\")\n",
    "    bleu_score = bleu.compute(predictions=preds_list, references=refs_list, max_order=4)\n",
    "    rouge_score = rouge.compute(predictions=preds_list, references=refs_list)\n",
    "    meteor_score = meteor.compute(predictions=preds_list, references=refs_list)\n",
    "\n",
    "    results = {\n",
    "        \"B1\": float(f\"{bleu_score['precisions'][0]:.3f}\"), \"B2\": float(f\"{bleu_score['precisions'][1]:.3f}\"),\n",
    "        \"B3\": float(f\"{bleu_score['precisions'][2]:.3f}\"), \"B4\": float(f\"{bleu_score['precisions'][3]:.3f}\"),\n",
    "        \"ROUGE-L\": float(f\"{rouge_score['rougeL']:.3f}\"), \"METEOR\": float(f\"{meteor_score['meteor']:.3f}\"),\n",
    "        \"CIDEr\": float(f\"{cider_score:.3f}\"), \"SPICE\": float(f\"{spice_score:.3f}\")\n",
    "    }\n",
    "    print(f\"Results for ENSEMBLE {split_name}: {results}\")\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73635496-d1d0-45f5-990e-9385e5d18f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_caption_with_ensemble(\n",
    "    image_path, encoder1, decoder1, mapping1, image_processor1,\n",
    "    encoder2, decoder2, mapping2, image_processor2, tokenizer\n",
    "):\n",
    "    print(\"\\n--- Uncertainty-based Fusion Ensemble Inference ---\")\n",
    "    print(f\"Image: {os.path.basename(image_path)}\") # FIX: Print the image file name\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    pixel_values1 = image_processor1(image, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "    pixel_values2 = image_processor2(image, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "\n",
    "    encoder1.eval(); decoder1.eval(); mapping1.eval()\n",
    "    encoder2.eval(); decoder2.eval(); mapping2.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        from transformers.modeling_outputs import BaseModelOutput\n",
    "        gen_kwargs = {\"max_length\": MAX_LENGTH, \"num_beams\": BEAM_WIDTH, \"return_dict_in_generate\": True, \"output_scores\": True}\n",
    "\n",
    "        encoder_outputs1 = encoder1(pixel_values=pixel_values1)\n",
    "        mapped1 = mapping1(encoder_outputs1.last_hidden_state)\n",
    "        encoder1_for_generate = BaseModelOutput(last_hidden_state=mapped1)\n",
    "        output1 = decoder1.generate(encoder_outputs=encoder1_for_generate, **gen_kwargs)\n",
    "\n",
    "        encoder_outputs2 = encoder2(pixel_values=pixel_values2)\n",
    "        mapped2 = mapping2(encoder_outputs2.last_hidden_state)\n",
    "        encoder2_for_generate = BaseModelOutput(last_hidden_state=mapped2)\n",
    "        output2 = decoder2.generate(encoder_outputs=encoder2_for_generate, **gen_kwargs)\n",
    "\n",
    "    caption1 = tokenizer.decode(output1.sequences[0], skip_special_tokens=True)\n",
    "    caption2 = tokenizer.decode(output2.sequences[0], skip_special_tokens=True)\n",
    "    score1 = output1.sequences_scores[0].item()\n",
    "    score2 = output2.sequences_scores[0].item()\n",
    "\n",
    "    print(f\"Swin-T5 Candidate: '{caption1}' (Confidence Score: {score1:.2f})\")\n",
    "    print(f\"ViT-T5  Candidate: '{caption2}' (Confidence Score: {score2:.2f})\")\n",
    "\n",
    "    final_caption = caption1 if score1 > score2 else caption2\n",
    "    print(f\"\\nâœ¨ Final Selected Caption: '{final_caption}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bcf8aa-0c2e-4258-b652-4417135701c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Main Execution ---\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt'); nltk.data.find('corpora/wordnet'); nltk.data.find('corpora/omw-1.4')\n",
    "    except (OSError, LookupError, BadZipFile):\n",
    "        print(\"NLTK data not found, downloading...\")\n",
    "        try:\n",
    "            nltk_data_path = os.path.join(nltk.data.path[0])\n",
    "            if os.path.exists(nltk_data_path): shutil.rmtree(nltk_data_path)\n",
    "        except Exception: pass\n",
    "        nltk.download('punkt', quiet=True); nltk.download('wordnet', quiet=True); nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "    decoder_config = AutoConfig.from_pretrained(DECODER_NAME)\n",
    "    decoder_config.dropout_rate = T5_DROPOUT_RATE\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DECODER_NAME)\n",
    "\n",
    "    # --- MODEL 1: Swin-T5 ---\n",
    "    print(\"--- Initializing Model 1: Swin-T5 ---\")\n",
    "    encoder_swin = SwinModel.from_pretrained(ENCODER_SWIN).to(DEVICE)\n",
    "    decoder_t5_1 = T5ForConditionalGeneration.from_pretrained(DECODER_NAME, config=decoder_config).to(DEVICE)\n",
    "    mapping_swin = MappingLayer(encoder_swin.config.hidden_size, decoder_t5_1.config.d_model).to(DEVICE)\n",
    "    image_processor_swin = AutoImageProcessor.from_pretrained(ENCODER_SWIN)\n",
    "\n",
    "    full_ds_swin = Flickr8kDataset(IMAGES_PATH, CAPTIONS_FILE, image_processor_swin, tokenizer)\n",
    "    total_size = len(full_ds_swin)\n",
    "    train_size, val_size, test_size = int(0.8 * total_size), int(0.1 * total_size), total_size - int(0.8 * total_size) - int(0.1 * total_size)\n",
    "    print(f\"\\n--- Dataset Split ---\\nTotal: {total_size}, Train: {train_size}, Val: {val_size}, Test: {test_size}\\n\")\n",
    "    train_indices, val_indices, test_indices = random_split(range(total_size), [train_size, val_size, test_size])\n",
    "    train_ds_swin = torch.utils.data.Subset(full_ds_swin, train_indices)\n",
    "    val_ds_swin = torch.utils.data.Subset(full_ds_swin, val_indices)\n",
    "    test_ds_swin = torch.utils.data.Subset(full_ds_swin, test_indices)\n",
    "\n",
    "    train_loader_swin = DataLoader(train_ds_swin, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader_swin = DataLoader(val_ds_swin, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn)\n",
    "    test_loader_swin = DataLoader(test_ds_swin, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn)\n",
    "\n",
    "    print(\"Fine-tuning ALL layers of the Swin encoder.\")\n",
    "    optimizer_swin = AdamW(list(encoder_swin.parameters()) + list(decoder_t5_1.parameters()) + list(mapping_swin.parameters()), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    train_model(encoder_swin, decoder_t5_1, mapping_swin, train_loader_swin, val_loader_swin, optimizer_swin, DEVICE, TOTAL_EPOCHS, \"Swin-T5\")\n",
    "\n",
    "    # --- MODEL 2: ViT-T5 ---\n",
    "    print(\"\\n--- Initializing Model 2: ViT-T5 ---\")\n",
    "    encoder_vit = ViTModel.from_pretrained(ENCODER_VIT).to(DEVICE)\n",
    "    decoder_t5_2 = T5ForConditionalGeneration.from_pretrained(DECODER_NAME, config=decoder_config).to(DEVICE)\n",
    "    mapping_vit = MappingLayer(encoder_vit.config.hidden_size, decoder_t5_2.config.d_model).to(DEVICE)\n",
    "    image_processor_vit = AutoImageProcessor.from_pretrained(ENCODER_VIT)\n",
    "\n",
    "    print(\"Fine-tuning ALL layers of the ViT encoder.\")\n",
    "    full_ds_vit = Flickr8kDataset(IMAGES_PATH, CAPTIONS_FILE, image_processor_vit, tokenizer)\n",
    "    train_ds_vit = torch.utils.data.Subset(full_ds_vit, train_indices)\n",
    "    val_ds_vit = torch.utils.data.Subset(full_ds_vit, val_indices)\n",
    "    test_ds_vit = torch.utils.data.Subset(full_ds_vit, test_indices)\n",
    "\n",
    "    train_loader_vit = DataLoader(train_ds_vit, batch_size=BATCH_SIZE, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    val_loader_vit = DataLoader(val_ds_vit, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn)\n",
    "    test_loader_vit = DataLoader(test_ds_vit, batch_size=BATCH_SIZE, collate_fn=custom_collate_fn)\n",
    "\n",
    "    optimizer_vit = AdamW(list(encoder_vit.parameters()) + list(decoder_t5_2.parameters()) + list(mapping_vit.parameters()), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    train_model(encoder_vit, decoder_t5_2, mapping_vit, train_loader_vit, val_loader_vit, optimizer_vit, DEVICE, TOTAL_EPOCHS, \"ViT-T5\")\n",
    "\n",
    "    # --- Final Ensemble Evaluation ---\n",
    "    evaluate_ensemble(\n",
    "        encoder_swin, decoder_t5_1, mapping_swin, image_processor_swin,\n",
    "        encoder_vit, decoder_t5_2, mapping_vit, image_processor_vit,\n",
    "        train_loader_swin, tokenizer, DEVICE, \"Train\"\n",
    "    )\n",
    "    evaluate_ensemble(\n",
    "        encoder_swin, decoder_t5_1, mapping_swin, image_processor_swin,\n",
    "        encoder_vit, decoder_t5_2, mapping_vit, image_processor_vit,\n",
    "        val_loader_swin, tokenizer, DEVICE, \"Validation\"\n",
    "    )\n",
    "    evaluate_ensemble(\n",
    "        encoder_swin, decoder_t5_1, mapping_swin, image_processor_swin,\n",
    "        encoder_vit, decoder_t5_2, mapping_vit, image_processor_vit,\n",
    "        test_loader_swin, tokenizer, DEVICE, \"Test\"\n",
    "    )\n",
    "\n",
    "    sample_image_path = os.path.join(IMAGES_PATH, full_ds_swin.image_files[test_ds_swin.indices[0]])\n",
    "    generate_final_caption_with_ensemble(\n",
    "        sample_image_path,\n",
    "        encoder_swin, decoder_t5_1, mapping_swin, image_processor_swin,\n",
    "        encoder_vit, decoder_t5_2, mapping_vit, image_processor_vit,\n",
    "        tokenizer\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
